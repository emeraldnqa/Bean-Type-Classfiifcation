{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "780f7177",
   "metadata": {},
   "source": [
    "# Bean Classification Project Proposal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbe45c1e-ecbb-4793-b803-00e624b6b055",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into ‘/home/jupyter/R/x86_64-pc-linux-gnu-library/4.2’\n",
      "(as ‘lib’ is unspecified)\n",
      "\n",
      "Installing package into ‘/home/jupyter/R/x86_64-pc-linux-gnu-library/4.2’\n",
      "(as ‘lib’ is unspecified)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "install.packages(\"cowplot\")\n",
    "install.packages(\"kknn\")\n",
    "library(kknn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf6aaebf-58ca-429f-8498-58ef04ba5b82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: ggplot2\n",
      "\n",
      "Registered S3 method overwritten by 'GGally':\n",
      "  method from   \n",
      "  +.gg   ggplot2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(GGally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31c75def",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into ‘/home/jupyter/R/x86_64-pc-linux-gnu-library/4.2’\n",
      "(as ‘lib’ is unspecified)\n",
      "\n",
      "Warning message in system(\"timedatectl\", intern = TRUE):\n",
      "“running command 'timedatectl' had status 1”\n",
      "── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────── tidyverse 2.0.0 ──\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdplyr    \u001b[39m 1.1.0     \u001b[32m✔\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr  \u001b[39m 1.5.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.2     \u001b[32m✔\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.1     \u001b[32m✔\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.0\n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in library(tidymodels): there is no package called ‘tidymodels’\n",
     "output_type": "error",
     "traceback": [
      "Error in library(tidymodels): there is no package called ‘tidymodels’\nTraceback:\n",
      "1. library(tidymodels)"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "install.packages(\"themis\")\n",
    "library(tidyverse)\n",
    "library(repr)\n",
    "library(tidymodels)\n",
    "options(repr.matrix.max.rows = 16)\n",
    "library(readxl)\n",
    "library(ggplot2)\n",
    "library(cowplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabf072f-15c9-47fd-bf2e-ccad20ee4dea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "library(themis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3be4dab",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This data set was extracted from the article written by Kolku and Ozkan, “Multiclass classification of dry beans using computer vision and machine learning techniques” (2020). Koluk and Ozkan (2020) explored the use of computer vision and machine learning techniques to classify dry beans into different classes. Based on the study's dataset, this report will also try to accurately classify the dry beans into their types based on the extracted features. This report is relevant for the food industry and agriculture, as it can potentially help in automating the classification process of dry beans, which is currently done manually and is time-consuming.\n",
    "\n",
    "**Research Question**: Can we accurately predict the bean type in an image based on the predictors Area and Roundness?\n",
    "\n",
    "This dataset is based on 13611 images of 7 types of individual dry beans with similar features. Each image was analyzed for 16 features of the bean (4 \"shape factors\", 12 structural/geometric features) (Kolku & Ozkan, 2020). While the dataset uses up to 12 predictors to classify the data points, we will try to reduce the number of the predictors and see if the accuracy of the estimate is still high or nearly as high as when using the full set of predictors.\n",
    "\n",
    "We will be using the K-nearest neighbours classification to predict \"Class\" using the mentioned predictors. K-nearest neighbours finds the K-closest data points to the input sample and assigns the most common class label amongst them. The accuracy of the model can be improved by tuning the value of K.\n",
    "\n",
    "\n",
    "Our variable of intetrest (Kolku & Ozkan, 2020).:\n",
    "- **Area**:The area of a bean zone and the number of pixels within its boundaries.\n",
    "- **Roundness**: Calculated with the following formula: (4piA)/(P^2). Where A is the area and P is the perimeter of the bean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad0a087",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4446a1c8-968a-4cd2-8849-e16a2329e7aa",
   "metadata": {},
   "source": [
    "**Loading Data**: A zip file was loaded from the original web source (https). Then, we created a temporary file path to store the *temp* variable, to temporarily store the downloaded zip file before it is extracted. Next, we used 'download.file' to download the zip file and saved it in 'temp'. The unzip() function extracts the xlsx file from the downloaded zip file and saves it in the 'beanzip' filepath. Finally, the read_excel file reads the xlsx file and we named the data frame 'bean'. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02201e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "beanurl<-\"https://archive.ics.uci.edu/ml/machine-learning-databases/00602/DryBeanDataset.zip\"\n",
    "temp <- tempfile()\n",
    "download.file(beanurl,temp)\n",
    "beanzip <- unzip(temp, \"DryBeanDataset/Dry_Bean_Dataset.xlsx\")\n",
    "bean <- read_excel(beanzip)\n",
    "bean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bca470-c90c-4f5e-b9c4-b577c896031d",
   "metadata": {},
   "source": [
    "*Table 1: Raw Bean dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61ee2bd",
   "metadata": {},
   "source": [
    "**Selecting variables & data wrangling & cleaning**: We will explore the data and and select the variables for prediction. The variables should produce distinct clusters by bean type, which is helpful in classifying the datapoints. \n",
    "\n",
    "To do so, we create a scatterplot matrix to further explore the relationship between all variables, and have the colors based on the \"Class\" variable. To do so, we use the \"ggpairs\" function. The \"columns\" argument includes all the variables we included in the matrix, and the \"lower\" argument specifies the type of plot. In this graph, we chose a \"continuous\" scatterplot and \"combo\" to display off-diagonal plots are dot plots without facets that is easier to visualize. \n",
    "\n",
    "We then use plot_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf28c725-da48-4b31-99ee-2de39d4c7b03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix_corr_bean <- ggpairs(bean, mapping = aes(color = Class), columns = c(\"Area\", \"Perimeter\", \"MajorAxisLength\",\"MinorAxisLength\",\"AspectRation\",\"Eccentricity\",\n",
    "                                                                            \"ConvexArea\",\"EquivDiameter\",\"Extent\",\"Solidity\",\"roundness\"),\n",
    "                           lower = list(continuous = wrap(\"points\", size = 0.4), combo = wrap(\"dot_no_facet\", alpha = 0.4)))+\n",
    "                            theme(text = element_text (size = 20))\n",
    "matrix_corr_bean <- matrix_corr_bean + theme(axis.text = element_text(size = 20))\n",
    "options(repr.plot.width = 30, repr.plot.height = 100)\n",
    "\n",
    "plot_grid( matrix_corr_bean[2,1], matrix_corr_bean[3,1],\n",
    "           matrix_corr_bean[3,2],\n",
    "           matrix_corr_bean[4,1],matrix_corr_bean[4,2], \n",
    "           matrix_corr_bean[4,3],\n",
    "           matrix_corr_bean[5,1],matrix_corr_bean[5,2],\n",
    "           matrix_corr_bean[5,3],matrix_corr_bean[5,4],\n",
    "           matrix_corr_bean[6,1],matrix_corr_bean[6,2],         \n",
    "           matrix_corr_bean[6,3],matrix_corr_bean[6,4],\n",
    "           matrix_corr_bean[6,5],matrix_corr_bean[7,1],\n",
    "           matrix_corr_bean[7,2],matrix_corr_bean[7,3],\n",
    "           matrix_corr_bean[7,4],matrix_corr_bean[7,5],\n",
    "           matrix_corr_bean[7,6],matrix_corr_bean[8,1],\n",
    "           matrix_corr_bean[8,2],matrix_corr_bean[8,3],\n",
    "           matrix_corr_bean[8,4],matrix_corr_bean[8,5],\n",
    "           matrix_corr_bean[8,6],matrix_corr_bean[8,7],\n",
    "           matrix_corr_bean[9,1],matrix_corr_bean[9,2],\n",
    "           matrix_corr_bean[9,3],matrix_corr_bean[9,4],\n",
    "           matrix_corr_bean[9,5],matrix_corr_bean[9,6],\n",
    "           matrix_corr_bean[9,7],matrix_corr_bean[9,8],\n",
    "           matrix_corr_bean[10,1],matrix_corr_bean[10,2],\n",
    "           matrix_corr_bean[10,3],matrix_corr_bean[10,4],\n",
    "           matrix_corr_bean[10,5],matrix_corr_bean[10,6],\n",
    "           matrix_corr_bean[10,7],matrix_corr_bean[10,8],\n",
    "           matrix_corr_bean[10,9],matrix_corr_bean[11,1],\n",
    "           matrix_corr_bean[11,2],matrix_corr_bean[11,3],\n",
    "           matrix_corr_bean[11,4],matrix_corr_bean[11,5],\n",
    "           matrix_corr_bean[11,6],matrix_corr_bean[11,7],\n",
    "           matrix_corr_bean[11,8],matrix_corr_bean[11,9],\n",
    "           matrix_corr_bean[11,10],\n",
    "           nrow = 16, ncol = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c754646-24ff-49db-82ba-e922e6dc26ae",
   "metadata": {},
   "source": [
    "Figure 0: Structural features of beans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33bdf0a-d21a-4e35-93af-0fc0a65b09fc",
   "metadata": {},
   "source": [
    "The \"ggpairs\" function helps us to better visualize the relationship between variables and their class. It can help us to identify any correlations, outliers, and class separation (see *Figure 0*). \n",
    "\n",
    "In order to determine the best set of data for analysis, we looked at the two groups that best represented the similarities between the bean classes as well as having the better cluster groups compared to the other variables. According to *Figure 0*, we will select the variables Area and Roundness. \n",
    "\n",
    "Then, we create a table only with the variables we're interested in: Area, roundness, and Class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "331ce777",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in select(bean, \"Area\", \"roundness\", \"Class\"): object 'bean' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in select(bean, \"Area\", \"roundness\", \"Class\"): object 'bean' not found\nTraceback:\n",
      "1. select(bean, \"Area\", \"roundness\", \"Class\")"
     ]
    }
   ],
   "source": [
    "select_bean_var <- bean |>\n",
    "                select(\"Area\",\"roundness\",\"Class\")\n",
    "select_bean_var "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fadd3cc-7e0b-4a85-a0fd-4856a837c283",
   "metadata": {},
   "source": [
    "*Table 2: Bean data frame with chosen variable*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85aca7e-63b1-4254-b522-620de4b2845f",
   "metadata": {},
   "source": [
    "**Visualize plot**: all of the bean types, in relation to roundness and area, are plotted on the graph to better understand the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fced6e-6c47-4dcd-a989-494ee24c6b81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width = 12, repr.plot.height = 7)\n",
    "bean_plot <- select_bean_var |>\n",
    "  ggplot(aes(x = Area, y = roundness, color = Class)) +\n",
    "  geom_point(size = 0.2) +\n",
    "  labs(x = \"Area\", \n",
    "       y = \"Roundness\",\n",
    "       color = \"Type\") +\n",
    "  ggtitle(\"Figure 1: Area and Roundness of all bean type\")+\n",
    "  theme(text = element_text(size = 12))+\n",
    "  guides(colour = guide_legend(override.aes = list(size=2)))\n",
    "bean_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6751431a-9e28-4583-953d-96ab02276926",
   "metadata": {},
   "source": [
    "As shown in *Figure 1*, Barbunya has a lack of clustering that causes too many overlaps. This may result in an inaccurate model. Thus, we will remove the Class Barbunya. which will enable the KNN model to better distinguish between the remaining classes, ultimately improving the overall accuracy of the model. The \"Class\" column was also converted to a factor type for this classification model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e201e08-ed42-4e10-8f37-9bb8f8c0d882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "select_bean <- select_bean_var|>\n",
    "                filter(Class != \"BARBUNYA\")|>\n",
    "                mutate(Class = as.factor(Class))\n",
    "select_bean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f4e9bc-ea4d-4607-bde6-022062fb9437",
   "metadata": {},
   "source": [
    "*Table 3: Bean dataframe with Barbunya filtered out*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c899e66-d3fe-4779-b4e8-f17fe318c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bean_plot2 <- select_bean |>\n",
    "  ggplot(aes(x = Area, y = roundness, color = Class)) +\n",
    "  geom_point(size = 0.2) +\n",
    "  labs(x = \"Area\", \n",
    "       y = \"Roundness\",\n",
    "       color = \"Type\") +\n",
    "  ggtitle(\"Figure 2: Area and Roundess of bean types (Barbunya filtered)\")+\n",
    "  theme(text = element_text(size = 12))\n",
    "bean_plot2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763198c4-020a-44f5-b57b-f200377343cb",
   "metadata": {},
   "source": [
    "We graph it again, and we can see that the clusters in *Figure 2* are more clean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9390cf65-c5b0-42f4-9fc7-13400d1ff45b",
   "metadata": {},
   "source": [
    "Next, we split the dataset into a training set and a testing set. The training set is used to build the model and make predictions on the testing set. By splitting the data, it can ensure that the model can be generalized to the new data, which is the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930fa6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating training and testing dataset\n",
    "\n",
    "set.seed(2022)\n",
    "bean_split <- initial_split(select_bean, prop = 0.75, strata = Class)\n",
    "bean_train <- training(bean_split)\n",
    "bean_test <- testing(bean_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3971d72c",
   "metadata": {},
   "source": [
    "Graphing the training dataset in plot point. *Figure 2* is very similar to *Figure 3*, which indicates a good distribution as the datasets were split randomly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953a0065",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_round_plot <- bean_train |>\n",
    "  ggplot(aes(x = Area, y = roundness, color = Class)) +\n",
    "  geom_point(size = 0.2) +\n",
    "  labs(x = \"Area\", \n",
    "       y = \"Roundness\",\n",
    "       color = \"Type\") +\n",
    "  ggtitle(\"Figure 3: Area and roundness of bean types in training dataset\")+\n",
    "  theme(text = element_text(size = 12))\n",
    "area_round_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ddf691",
   "metadata": {},
   "source": [
    "From *Figure 3*, we can see that there is a big difference between the area value and the roundness value. Therefore, we will scale the data, which will be dealt with in `workflow()` . Next, we summarized the training dataset to visualize the distribution of bean types. We grouped the beans by their types (Class) , summarized by their counts, and created a new column called *percentage_dist* to find the percentage of each bean type in the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c055da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize training dataset\n",
    "bean_class_dist <- bean_train |>\n",
    "                group_by(Class)|>\n",
    "                summarize(count = n()) |>\n",
    "                mutate(percentage_dist = 100*count/nrow(bean_train))\n",
    "bean_class_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0045b50c-0708-40a0-846e-fa971aa5d4f7",
   "metadata": {},
   "source": [
    "*Table 4: Distribution of bean types in training set*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756e4642",
   "metadata": {},
   "source": [
    "*Table 4* and *Figure 4* shows us the distribution of bean types in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4877e4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bean_class_dist_plot <- bean_class_dist |>\n",
    "                        ggplot(aes(x=Class, y = count))+\n",
    "                        geom_bar(stat = \"identity\")+\n",
    "                        labs(x= \"Type of beans\",\n",
    "                             y = \"Number of beans\")+\n",
    "                        ggtitle(\"Figure 4: Distribution of bean type in the training dataset\")+\n",
    "                        theme(text = element_text(size = 12))\n",
    "bean_class_dist_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdcc8b2",
   "metadata": {},
   "source": [
    "**Upsampling**: By looking at the *Table 4* and *Figure 4* viewing the total number of datapoints for each type of beans, we can see that there's a big difference in the number of datapoints between Dermason bean type compared to other type of beans. Therefore, we will upsample the training dataset so that each Class of bean has a comparable voting power when it comes to the classfication of the testing dataset. This is done in the 'recipe' and 'step_upsample ()' functions. \n",
    "\n",
    "**Scaling:** all data is called to avoid features with large values dominating the decision process. The scaling of the data will later be part of the Classifier building process called `workflow()`.\n",
    "\n",
    "Next, we plot the data agin to see what the upsampled data set looks like. We can see that each cluster should have approximately equal data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1616d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scalling all data:\n",
    "\n",
    "bean_data_training_scaled_recipe <- recipe(Class ~., data = bean_train) |>\n",
    "                        step_upsample()|>\n",
    "                        prep()\n",
    "\n",
    "final_bean_data <- bake(bean_data_training_scaled_recipe, bean_train)\n",
    "\n",
    "area_round_plot_scaled <- final_bean_data |>\n",
    "  ggplot(aes(x = Area, y = roundness, color = Class)) +\n",
    "  geom_point(size = 0.2) +\n",
    "  labs(x = \"Area\", \n",
    "       y = \"Roundness\",\n",
    "       color = \"Type\") +\n",
    "  ggtitle(\"Figure 5: Area and roundness in training dataset (upsampled)\")+\n",
    "  theme(text = element_text(size = 12))\n",
    "area_round_plot_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e17637",
   "metadata": {},
   "source": [
    "**Summary of the data set**: Next we will look at Statistical distribution of our chosen variables, in table and boxplot form. To understand the distribution, we calculate the basic statistics such as range and standard deviation. To do this, we group the beans by their types (Class), and created a new dataframe that includes the statistics in each column. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d0d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistical Distribution of predictor based on class\n",
    "features_dist_by_class <- final_bean_data |>\n",
    "                group_by(Class)|>\n",
    "                summarize(max_area = max(Area, na.rm = TRUE),\n",
    "                          min_area = min(Area, na.rm = TRUE),\n",
    "                          std_dev_area = sd(Area, na.rm = TRUE),\n",
    "                          max_roundness = max(roundness, na.rm = TRUE),\n",
    "                          min_roundness = min(roundness, na.rm = TRUE),\n",
    "                         std_dev_roundness = sd(roundness, na.rm = TRUE))\n",
    "features_dist_by_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18919fd2-7340-401d-b2fd-759d413d2657",
   "metadata": {},
   "source": [
    "*Table 5: Statistical distribution of each predictor based on their class*\n",
    "\n",
    "Next, we create *Table 6* to explore the statistical distribution of the \"Area\" and \"roundness\".   First, we select these to variables and reshape the data using 'pivot_longer()' to make it tidy. We then group the data by the \"Features\" column and calculate summary statistics for each feature, including the mean, minimum, maximum, and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb058cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistical Distribution in Features of varieties of dry bean\n",
    "features_dist <- bean_train |>\n",
    "            select(Area,roundness) |>\n",
    "            pivot_longer(cols= Area:roundness,\n",
    "                         names_to = \"Features\",\n",
    "                         values_to = \"values\") |>\n",
    "            group_by(Features) |>\n",
    "            summarize(Mean = mean(values, na.rm = TRUE),\n",
    "                      Min = min(values, na.rm = TRUE),\n",
    "                      Max = max(values, na.rm = TRUE),\n",
    "                     Std_Deviation = sd(values, na.rm = TRUE))\n",
    "features_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df45d05-f5e0-43f7-8bd9-46d2610fe4ff",
   "metadata": {},
   "source": [
    "*Table 6: Statistical distribution of the predictors in the whole dataset*\n",
    "\n",
    "Then, to learn more about the distribution, we graph the area distribution for each type of bean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf1954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "area_box_plot <- final_bean_data |>\n",
    "                ggplot(aes(x = Class, y = Area))+\n",
    "                geom_boxplot()+\n",
    "                xlab(\"Type of beans\")+\n",
    "                ylab(\"Area\")+\n",
    "                ggtitle(\"Figure 6: Area Distribution for each type of bean\")+\n",
    "                coord_flip()\n",
    "area_box_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e5633e-86e2-4705-a92e-5de02de90890",
   "metadata": {},
   "source": [
    "By looking at *Figure 5*, we can find that the square boxes, which represents the middle 50% of each bean type, have little overlap over each other (Krzywinski et al., 2014) . We can also find that 5/6 bean types (Sira, Seker, Horoz, Cali, and Bombay) have outliers that are higher in value than most of its usual area size, based on the data points above the upper whiskers (Krzywinski et al., 2014). We might expect that these outliers might also affect the accuracy of the classifier as these outliers overlap other bean types area measurements. However, out of all the bean types, Bombay seems to have the most distinct area distribution, so we might see that the classifier can predict the Bombay bean type better than other types of beans. Sira and Seker area distributions are seen overlapping each other, so we also expect this to affect classifier accuracy.\n",
    "\n",
    "Then, we graph the box plot showing roundness distribution for each type of box plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd7c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "roundness_box_plot <- final_bean_data |>\n",
    "                ggplot(aes(x = Class, y = roundness))+\n",
    "                geom_boxplot()+\n",
    "                xlab(\"Type of beans\")+\n",
    "                ylab(\"Roundess\")+\n",
    "                ggtitle(\"Figure 7: Roundness distribution for each type of bean\")+\n",
    "                coord_flip()\n",
    "roundness_box_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3deea9-7925-42cd-b130-cd0810e0df44",
   "metadata": {},
   "source": [
    "In *Figure 7*, we can see that most of the bean types' middle 50% of roundness distribution do not overlap each other. However, there are a lot of outliers that cause these bean type roundness distribution to overlap each other. The overlap of roundness distribution of dataset matches the distribution of bean type of Area and Roundness that we see in Figure 1, 2, 3, 5. However, due to 50% of the dataset mostly don't overlap each other as we seen in *Figure 7*, we can also see distinct cluster in Figure 1, 2, 3, and 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765d3350",
   "metadata": {},
   "source": [
    "# Building the Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e289d843-d2d9-4309-853d-777aabb6660c",
   "metadata": {},
   "source": [
    "Now we should start building the classifier. However, before starting to build the classifier, we need to create the scaling and centering recipe on the training dataset, to ensure that all predictors are standardized, so that predictor with larger scales won't create a greater unwanted affect. We set the seed to 2022 so the codes can be reproducible, and it produces consistent results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5445092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2022) # DO NOT REMOVE\n",
    "\n",
    "bean_report_recipe <- recipe(Class ~., data = final_bean_data) |>\n",
    "    step_scale(all_predictors()) |>\n",
    "    step_center(all_predictors())\n",
    "bean_report_recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50e9394-403f-406e-a3a7-791df2f7daa7",
   "metadata": {},
   "source": [
    "Next, we need to tune our model using the cross-validation method, so that we can choose the optimal K-neighbour. First we need to specify the model for cross-validation, where the neighbour = tune() is used to test accuracies of model across different range of k-neighbours. Cross-validation is a method to help us tune our classifier. Cross-validation will randomly divide the training sets into specified number of smaller set with the same size. The method then will train our classifier using the remaining sets that were not accept as one test set. This process is repeated until all of the set that was divided has a chance to be a test set (Arlot and Celisse, 2010). Here, we use cross-validation across a range of number of neighbours and get the number with the best accuracy estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836fbb2-2f68-49d5-814b-7234d7e318c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2022) # DO NOT REMOVE\n",
    "\n",
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) |>\n",
    "  set_engine(\"kknn\") |>\n",
    "  set_mode(\"classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f57501-cb1e-4774-9fa7-587685a56a20",
   "metadata": {},
   "source": [
    "Now we use the `vfold_cv()` to split the training data into 10 fold for cross-validation. `gridvals` is where we set the range of k-neighbours that we would like to cross-validate with. Since we have run the this code a few time to get the best estimate of a range of k-neighbours to use, we decide only run upto 30 k-neighbours to get the best k-neighbours for our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86e8da-df67-49f2-93b2-1e8cb18309f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set.seed(2022) # DO NOT REMOVE\n",
    "bean_vfold <- vfold_cv(final_bean_data, v = 10, strata = Class)\n",
    "gridvals <- tibble(neighbors = seq(1,30,by=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052aba22-7ced-4ca9-9150-7f5518a1ce88",
   "metadata": {},
   "source": [
    "Now we start to cross-validate multiple k-neighbours to the training dataset. The function `tune_grid()` here allow us to fit the model for each value in a range of value. After the cross-validation is run, we collect the accuracies calculated from each value of k-neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4106185b-8b95-488c-b82c-f853ab337c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2022) # DO NOT REMOVE\n",
    "\n",
    "knn_results <- workflow() |>\n",
    "  add_recipe(bean_report_recipe) |>\n",
    "  add_model(knn_spec) |>\n",
    "  tune_grid(resamples = bean_vfold, grid = gridvals) |>\n",
    "  collect_metrics() |>\n",
    "  filter(.metric == \"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc0743-4b49-4521-a93a-d57f1cbb73b6",
   "metadata": {},
   "source": [
    "Now we graph the k-neighbours against the mean accuracy calculated from cross-validated across 10 folds to find the best k to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6c445-6125-4541-a594-ea3b1ab2b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width = 12, repr.plot.height = 7)\n",
    "\n",
    "# Plot k values against their respective accuracies and choose optimal k value\n",
    "cross_val_plot <- knn_results |> \n",
    "    ggplot(aes(x = neighbors, y = mean)) +\n",
    "    geom_point() +\n",
    "    geom_line() +\n",
    "    labs(x = \"Neighbors\", y = \"Accuracy Estimate\") +\n",
    "    ggtitle(\"Figure 9: K-Neighbours and their accuracy estimates\")+\n",
    "    scale_x_continuous(breaks = seq(1,30, by = 1))\n",
    "cross_val_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fab009f-c456-4b35-9290-7f34299f52cc",
   "metadata": {},
   "source": [
    "From *Figure 9*, we can see that with such a large dataset like the bean dataset, large number of neighbour is required to predict the data accurately. \n",
    "From the graph we can see that the highest accuracies is around 17-19 neighbours. We can also see that around 18 neighbors is a good K, since the accuracy does not fluctuate much between 17-19 neighbours "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69ccb0b-07a1-4afb-9733-10f45fad0a9a",
   "metadata": {},
   "source": [
    "Now that we have a good estimate of which neighbours yield the highest accuracy for our classification, we should build our classifier with that k-neighbour, in our case is: 18 neighbour. We first build the specification of the classifier using the best neighbour. Then we build the workflow with the best spec, and we can use the recipe that we created before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e8d60-a297-4978-ba7d-ef528d0d6064",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_best_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 18) |>\n",
    "  set_engine(\"kknn\") |>\n",
    "  set_mode(\"classification\")\n",
    "\n",
    "bean_fit <- workflow() |>\n",
    "  add_recipe(bean_report_recipe) |>\n",
    "  add_model(knn_best_spec) |>\n",
    "  fit(data = final_bean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d72091-2227-41e3-885a-b5d73bad185b",
   "metadata": {},
   "source": [
    "Now we start using the new classifier that we just created to build predict our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c53e8c-e52d-45a8-9399-031be5a117e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prediction column\n",
    "bean_predictions <- predict(bean_fit, bean_test) |> \n",
    "    bind_cols(bean_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a1848-e4f3-4914-81a7-f07ca21fc2df",
   "metadata": {},
   "source": [
    "Next, we calculate the accuracy of the classifier by comparing the predictions to the test set. the `metrics()` function calculates the performance of the model's predictions given the true Class and the predicted class labels. `Head(1)` then returns the first row of the data frame that contains the perfmance metric and its estimate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab236f3f-d2f8-4ba8-a12e-a066837edcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "bean_acc <- bean_predictions |> \n",
    "    metrics(truth = Class, estimate = .pred_class) |> \n",
    "    select(.metric, .estimate) |> \n",
    "    head(1)\n",
    "bean_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bac7fc3-7bae-47b7-a6f5-866a4d0c6084",
   "metadata": {},
   "source": [
    "*Table 7: Accuracy estimate our model against the test set*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77952a93-8984-4248-8b51-e9e1441f46e1",
   "metadata": {},
   "source": [
    "As we can see from *Table 7*, the accuracy the result of the k-nearest neighbors (k-NN) model shows an estimated accuracy of 0.8812622."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7fbe6a-bb7a-4e3e-9eea-d9ffd8ee90c3",
   "metadata": {},
   "source": [
    "**Visualizations of the analysis**: Now, we build a confusion matrix that compare the prediction made to the truth values. A confusion matrix is shown below to help interpret the results and communicate the findings effectively. The confusion matrix is also used to evaluate the performance of our model, and provides us with the accuracy percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbd2962-4e4d-44fd-ba6c-d91b2ff154a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bean_cm <- bean_predictions |> \n",
    "    conf_mat(truth = Class, estimate = .pred_class)\n",
    "bean_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02b9ab7-b71b-4b82-ad64-8fc60b149d5c",
   "metadata": {},
   "source": [
    "*Table 8: Confusion matrix of the predicted class and the truth class*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4957c1f-34cb-4fba-b4fb-9cf5a828093c",
   "metadata": {},
   "source": [
    "From *Table 8*, we can see the the number of true positives, true negatives, false positives, and false negatives for each class label. The diagonal of the matrix represents the correctly classified instances for each class label. For example, if we look at the first row - the BOMBAY class. All 141 instances that were predicted to belong to the BOMBAY class were actually labeled as BOMBAY, so it has 100% accuracy for that class. To calculate the accuracy of the model, we divide the diagonal of the matrix by all of the numbers, which is 2709/3074, and is 0.8812622, consistent with our results from *Table 7*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6f80cc-016d-48ec-a196-648541d6ed38",
   "metadata": {},
   "source": [
    "Finally, we want a visual comparison of true class vs predicted class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d827405f-3648-46b8-9185-a2f8ad1d864c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "bean_predictions_plot <- bean_predictions |>\n",
    "    ggplot(aes(x = Area, y = roundness, color = .pred_class)) +\n",
    "    geom_point(size = 0.3) +\n",
    "    labs(x = \"Area of Bean\", y = \"Roundness of Bean\", color = \"Type of Bean\") +\n",
    "    ggtitle(\"Figure 10: Predictions of Bean Type\")\n",
    "\n",
    "bean_trueclass_plot <- bean_predictions |>\n",
    "    ggplot(aes(x = Area, y = roundness, color = Class)) +\n",
    "    geom_point(size = 0.3) +\n",
    "    labs(x = \"Area of Bean\", y = \"Roundness of Bean\", color = \"Type of Bean\") +\n",
    "    ggtitle(\"Figure 11: True Class of Beans\")\n",
    "\n",
    "plot_grid(bean_predictions_plot, bean_trueclass_plot, ncol = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc5b215-0229-4adc-a811-ad58352d1f9f",
   "metadata": {},
   "source": [
    "We can see that *Figure 10* and *Figure 11* are very similar, which is consistent with our model accuracy of 88.13%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c33b507",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "**Results**\n",
    "\n",
    "With the help of a classification model, we expect to find a well-performing model that can predict the type of beans based on their roundness and area with at least 85% accuracy. From *Table 7*, the result of the k-nearest neighbors (k-NN) model shows an estimated accuracy of 0.8812622. This means that the model was able to correctly classify 88.13% of the beans in the dataset based on their area and roundness. This is a relatively high accuracy as the classes are similar in appearance and it's difficult to differentiate based on these features alone. While this estimated accuracy is high, if we use all the predictor that was proposed in the paper that the dataset was based on, the KNN algorithm will produce a classifier with a higher accuracy estimate (exactly 92.52% in the paper). \n",
    "\n",
    "Looking at each individual class prediction, in *Table 8*, we can see that the classifier predicts the Bombay bean type correctly 100% of the time. While the classifier predicts the Sira bean type the least accurately, if we take the number of predictions that were correctly divided by the number of predictions from *Table 8*, we can see that the classifier only predicts the Sira bean type correct 82% of the time, whereas the classifier predicts other bean types correctly more than 84% of the time. We can see the same result between *Figure 10* and 11, where the classifier cannot predict Sira bean type that has roundness and area measurements that are similar to the Horoz bean type.\n",
    "\n",
    "The model can correctly classify 88.13% of the data. To improve the accuracy of the model, we can increase the training data or collect more data. Using more data to train the model can help it lean and better generalize the model on new data. We can also add more predictor variables in the model and explore how that might change the accuracy of the model.  \n",
    "\n",
    "**Findings Impact**\n",
    "\n",
    "This classification model can impact the food and agriculture industry. \n",
    "\n",
    "An automated system can be created to classifiy the type of bean based on their physical appearance. This is espcially useful when food and agriculture companies collect different types of beans together, dry them, and want to package them based on different types. The model we built can help to streamline the service, gain quality control and reduce the time for people to manually separate the beans. \n",
    "\n",
    "**Future research questions**\n",
    "- Can other variables such as major axis length or the perimeter be good predictors of the type? And are they better predictors than roundness and area?\n",
    "- Can the classification model be used for different types of crops such as rice? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dd2864",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "- Arlot, S., & Celisse, A. (2010). *A survey of cross-validation procedures for model selection*.\n",
    "\n",
    "- Koklu, M., & Ozkan, I. A. (2020). Multiclass classification of dry beans using computer vision and machine learning techniques. Computers and Electronics in Agriculture, 174, 105507. https://doi.org/10.1016/j.compag.2020.105507\n",
    "\n",
    "- Krzywinski, Martin, and Naomi Altman. \"Visualizing samples with box plots: use box plots to illustrate the spread and differences of samples.\" Nature Methods, vol. 11, no. 2, Feb. 2014, pp. 119+. Gale OneFile: Health and Medicine, link.gale.com/apps/doc/A361242515/HRCA?u=ubcolumbia&sid=bookmark-HRCA&xid=0db0fe06. Accessed 11 Mar. 2023.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
